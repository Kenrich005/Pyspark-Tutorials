{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Handling Missing Values\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameter in Dropping functionalities\n",
    "- Handling Missing values by mean, median and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder. appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|YearsExperience| Age|Salary|\n",
      "+---------------+----+------+\n",
      "|            1.1|21.0| 39343|\n",
      "|            1.3|21.5| 46205|\n",
      "|            1.5|21.7| 37731|\n",
      "|            2.0|22.0| 43525|\n",
      "|            2.2|22.2| 39891|\n",
      "|            2.9|23.0| 56642|\n",
      "|            3.0|23.0| 60150|\n",
      "|            3.2|23.3| 54445|\n",
      "|            3.2|23.3| 64445|\n",
      "|            3.7|23.6| 57189|\n",
      "|            3.9|23.9| 63218|\n",
      "|            4.0|24.0| 55794|\n",
      "|            4.0|null| 56957|\n",
      "|            4.1|24.0| 57081|\n",
      "|            4.5|null| 61111|\n",
      "|            4.9|25.0| 67938|\n",
      "|            5.1|26.0| 66029|\n",
      "|           null|27.0| 83088|\n",
      "|           null|28.0| 81363|\n",
      "|            6.0|29.0| 93940|\n",
      "+---------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('wages with na.csv',header = True, inferSchema = True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|YearsExperience| Age|Salary|\n",
      "+---------------+----+------+\n",
      "|            1.1|21.0| 39343|\n",
      "|            1.3|21.5| 46205|\n",
      "|            1.5|21.7| 37731|\n",
      "|            2.0|22.0| 43525|\n",
      "|            2.2|22.2| 39891|\n",
      "|            2.9|23.0| 56642|\n",
      "|            3.0|23.0| 60150|\n",
      "|            3.2|23.3| 54445|\n",
      "|            3.2|23.3| 64445|\n",
      "|            3.7|23.6| 57189|\n",
      "|            3.9|23.9| 63218|\n",
      "|            4.0|24.0| 55794|\n",
      "|            4.1|24.0| 57081|\n",
      "|            4.9|25.0| 67938|\n",
      "|            5.1|26.0| 66029|\n",
      "|            6.0|29.0| 93940|\n",
      "|            6.8|30.0| 91738|\n",
      "|            7.1|30.0| 98273|\n",
      "|            7.9|31.0|101302|\n",
      "|            8.2|32.0|113812|\n",
      "+---------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing all rows with any null value\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|YearsExperience| Age|Salary|\n",
      "+---------------+----+------+\n",
      "|            1.1|21.0| 39343|\n",
      "|            1.3|21.5| 46205|\n",
      "|            1.5|21.7| 37731|\n",
      "|            2.0|22.0| 43525|\n",
      "|            2.2|22.2| 39891|\n",
      "|            2.9|23.0| 56642|\n",
      "|            3.0|23.0| 60150|\n",
      "|            3.2|23.3| 54445|\n",
      "|            3.2|23.3| 64445|\n",
      "|            3.7|23.6| 57189|\n",
      "|            3.9|23.9| 63218|\n",
      "|            4.0|24.0| 55794|\n",
      "|            4.0|null| 56957|\n",
      "|            4.1|24.0| 57081|\n",
      "|            4.5|null| 61111|\n",
      "|            4.9|25.0| 67938|\n",
      "|            5.1|26.0| 66029|\n",
      "|           null|27.0| 83088|\n",
      "|           null|28.0| 81363|\n",
      "|            6.0|29.0| 93940|\n",
      "+---------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drops rows where all values are null values\n",
    "df_pyspark.na.drop(how = \"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|YearsExperience| Age|Salary|\n",
      "+---------------+----+------+\n",
      "|            1.1|21.0| 39343|\n",
      "|            1.3|21.5| 46205|\n",
      "|            1.5|21.7| 37731|\n",
      "|            2.0|22.0| 43525|\n",
      "|            2.2|22.2| 39891|\n",
      "|            2.9|23.0| 56642|\n",
      "|            3.0|23.0| 60150|\n",
      "|            3.2|23.3| 54445|\n",
      "|            3.2|23.3| 64445|\n",
      "|            3.7|23.6| 57189|\n",
      "|            3.9|23.9| 63218|\n",
      "|            4.0|24.0| 55794|\n",
      "|            4.0|null| 56957|\n",
      "|            4.1|24.0| 57081|\n",
      "|            4.5|null| 61111|\n",
      "|            4.9|25.0| 67938|\n",
      "|            5.1|26.0| 66029|\n",
      "|           null|27.0| 83088|\n",
      "|           null|28.0| 81363|\n",
      "|            6.0|29.0| 93940|\n",
      "+---------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where at least two (thresh) non-null values exist\n",
    "df_pyspark.na.drop(how = \"any\", thresh = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|YearsExperience| Age|Salary|\n",
      "+---------------+----+------+\n",
      "|            1.1|21.0| 39343|\n",
      "|            1.3|21.5| 46205|\n",
      "|            1.5|21.7| 37731|\n",
      "|            2.0|22.0| 43525|\n",
      "|            2.2|22.2| 39891|\n",
      "|            2.9|23.0| 56642|\n",
      "|            3.0|23.0| 60150|\n",
      "|            3.2|23.3| 54445|\n",
      "|            3.2|23.3| 64445|\n",
      "|            3.7|23.6| 57189|\n",
      "|            3.9|23.9| 63218|\n",
      "|            4.0|24.0| 55794|\n",
      "|            4.1|24.0| 57081|\n",
      "|            4.9|25.0| 67938|\n",
      "|            5.1|26.0| 66029|\n",
      "|           null|27.0| 83088|\n",
      "|           null|28.0| 81363|\n",
      "|            6.0|29.0| 93940|\n",
      "|            6.8|30.0| 91738|\n",
      "|            7.1|30.0| 98273|\n",
      "+---------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset - Deleting rows value in null in Age column\n",
    "df_pyspark.na.drop(how = \"any\", subset = ['Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|YearsExperience| Age|Salary|\n",
      "+---------------+----+------+\n",
      "|            1.1|21.0| 39343|\n",
      "|            1.3|21.5| 46205|\n",
      "|            1.5|21.7| 37731|\n",
      "|            2.0|22.0| 43525|\n",
      "|            2.2|22.2| 39891|\n",
      "|            2.9|23.0| 56642|\n",
      "|            3.0|23.0| 60150|\n",
      "|            3.2|23.3| 54445|\n",
      "|            3.2|23.3| 64445|\n",
      "|            3.7|23.6| 57189|\n",
      "|            3.9|23.9| 63218|\n",
      "|            4.0|24.0| 55794|\n",
      "|            4.0| 0.0| 56957|\n",
      "|            4.1|24.0| 57081|\n",
      "|            4.5| 0.0| 61111|\n",
      "|            4.9|25.0| 67938|\n",
      "|            5.1|26.0| 66029|\n",
      "|            0.0|27.0| 83088|\n",
      "|            0.0|28.0| 81363|\n",
      "|            6.0|29.0| 93940|\n",
      "+---------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling missing value\n",
    "df_pyspark.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|YearsExperience| Age|Salary|\n",
      "+---------------+----+------+\n",
      "|            1.1|21.0| 39343|\n",
      "|            1.3|21.5| 46205|\n",
      "|            1.5|21.7| 37731|\n",
      "|            2.0|22.0| 43525|\n",
      "|            2.2|22.2| 39891|\n",
      "|            2.9|23.0| 56642|\n",
      "|            3.0|23.0| 60150|\n",
      "|            3.2|23.3| 54445|\n",
      "|            3.2|23.3| 64445|\n",
      "|            3.7|23.6| 57189|\n",
      "|            3.9|23.9| 63218|\n",
      "|            4.0|24.0| 55794|\n",
      "|            4.0| 0.0| 56957|\n",
      "|            4.1|24.0| 57081|\n",
      "|            4.5| 0.0| 61111|\n",
      "|            4.9|25.0| 67938|\n",
      "|            5.1|26.0| 66029|\n",
      "|           null|27.0| 83088|\n",
      "|           null|28.0| 81363|\n",
      "|            6.0|29.0| 93940|\n",
      "+---------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values in Age column\n",
    "df_pyspark.na.fill(0,'Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+\n",
      "|YearsExperience| Age|Salary|\n",
      "+---------------+----+------+\n",
      "|            1.1|21.0| 39343|\n",
      "|            1.3|21.5| 46205|\n",
      "|            1.5|21.7| 37731|\n",
      "|            2.0|22.0| 43525|\n",
      "|            2.2|22.2| 39891|\n",
      "|            2.9|23.0| 56642|\n",
      "|            3.0|23.0| 60150|\n",
      "|            3.2|23.3| 54445|\n",
      "|            3.2|23.3| 64445|\n",
      "|            3.7|23.6| 57189|\n",
      "|            3.9|23.9| 63218|\n",
      "|            4.0|24.0| 55794|\n",
      "|            4.0| 0.0| 56957|\n",
      "|            4.1|24.0| 57081|\n",
      "|            4.5| 0.0| 61111|\n",
      "|            4.9|25.0| 67938|\n",
      "|            5.1|26.0| 66029|\n",
      "|            0.0|27.0| 83088|\n",
      "|            0.0|28.0| 81363|\n",
      "|            6.0|29.0| 93940|\n",
      "+---------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling null with 0 in multiple columns\n",
    "df_pyspark.na.fill(0,['Age', 'YearsExperience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null value with mean\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer  = Imputer(\n",
    "inputCols = ['Age','YearsExperience','Salary'],\n",
    "outputCols = [\"{}_imputed\".format(c) for c in ['Age','YearsExperience','Salary']]\n",
    ").setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+-----------+-----------------------+--------------+\n",
      "|YearsExperience| Age|Salary|Age_imputed|YearsExperience_imputed|Salary_imputed|\n",
      "+---------------+----+------+-----------+-----------------------+--------------+\n",
      "|            1.1|21.0| 39343|       21.0|                    1.1|         39343|\n",
      "|            1.3|21.5| 46205|       21.5|                    1.3|         46205|\n",
      "|            1.5|21.7| 37731|       21.7|                    1.5|         37731|\n",
      "|            2.0|22.0| 43525|       22.0|                    2.0|         43525|\n",
      "|            2.2|22.2| 39891|       22.2|                    2.2|         39891|\n",
      "|            2.9|23.0| 56642|       23.0|                    2.9|         56642|\n",
      "|            3.0|23.0| 60150|       23.0|                    3.0|         60150|\n",
      "|            3.2|23.3| 54445|       23.3|                    3.2|         54445|\n",
      "|            3.2|23.3| 64445|       23.3|                    3.2|         64445|\n",
      "|            3.7|23.6| 57189|       23.6|                    3.7|         57189|\n",
      "|            3.9|23.9| 63218|       23.9|                    3.9|         63218|\n",
      "|            4.0|24.0| 55794|       24.0|                    4.0|         55794|\n",
      "|            4.0|null| 56957|       25.0|                    4.0|         56957|\n",
      "|            4.1|24.0| 57081|       24.0|                    4.1|         57081|\n",
      "|            4.5|null| 61111|       25.0|                    4.5|         61111|\n",
      "|            4.9|25.0| 67938|       25.0|                    4.9|         67938|\n",
      "|            5.1|26.0| 66029|       26.0|                    5.1|         66029|\n",
      "|           null|27.0| 83088|       27.0|                    4.1|         83088|\n",
      "|           null|28.0| 81363|       28.0|                    4.1|         81363|\n",
      "|            6.0|29.0| 93940|       29.0|                    6.0|         93940|\n",
      "+---------------+----+------+-----------+-----------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the imputation to columns of the df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
